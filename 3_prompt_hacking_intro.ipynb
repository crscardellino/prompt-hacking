{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66a1df5-3922-49f9-a310-588ae92e7c5c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"text-align:center;font-weight:normal;font-size:75pt;\">Prompt Hacking 101: Prompt Injection vs. Jailbreaking</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b4ee12-8f7d-4ebd-b951-313ef3bb3711",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Prompt Injection/Hijacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658d4103-2bc9-439b-8e70-59c660048ced",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## ¿Qué es el Prompt Injection/Hijacking?\n",
    "\n",
    "* Es un ataque a aplicaciones que hacen uso de LLMs.\n",
    "* Las aplicaciones necesariamente deben concatenar un prompt *trusted* (escrito por el desarrollador) a un prompt *untrusted* (escrito por el usuario).\n",
    "* El concepto es análogo a SQL injection, donde el código SQL del desarrollador es concatenado a un input del usuario.\n",
    "* Otra forma en la que se denomina a este concepto es _Prompt Hijacking_.\n",
    "    * Esto es porque, si bien el término original fue [acuñado por Simon Willinson](https://simonwillison.net/2022/Sep/12/prompt-injection/) para hacer referencia sólo a este tipo de ataque (i.e., que concatenan prompt con input del usuario), el concepto se fue mezclando con el concepto de Jailbreaking. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2409c7-4f3f-4465-91ad-cdbab92aa6bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Ejemplo\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <div style=\"display:inline-block;margin-right:20px;\">\n",
    "        <img src=\"./img/prompt-injection.png\" style=\"height:30em;width:auto;\"/>\n",
    "    </div>\n",
    "    <h6 style=\"font-style:normal;font-size:1em;margin:5px;\">\n",
    "        Source: <a href=\"https://x.com/remoteli_io/status/1570547034159042560\" style=\"color:royalblue;\" target=\"_blank\">@remoteli_io</a>\n",
    "    </h6>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e7144e-9f4f-43ca-bf03-0718cf70655c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Peligros del Prompt Injection\n",
    "\n",
    "* El prompt injection está ligado a las características de la aplicación que se está atacando.\n",
    "* Se busca acceder a la información a la que la aplicación puede acceder (bases de datos, cómputo, etc.)\n",
    "* Muchas de estas aplicaciones dependen de equipos más chicos y/o desarrolladores independientes.\n",
    "    * Para evitarlos se debe asegurar que el LLM seguirá las instrucciones del desarrollador, no las del usuario.\n",
    "* Aplicaciones con acceso a datos confidenciales y que usan LLMs son vulnerables a estos ataques.\n",
    "    * Ejemplo: Sistemas con agentes que accedan a bases de datos u otras herramientas.\n",
    "* Es un caso más real de ataque y representa una vulnerabilidad en todos los sistemas construídos sobre LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b684b-b857-410f-a5cf-672c9abb3d09",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Exfiltración de Datos (Data Exfiltration)\n",
    "\n",
    "* Manipula la IA para que extraiga datos confidenciales.\n",
    "* Se basa en los accesos a herramientas externas (e.g., bases de datos, servidores de emails, etc.)\n",
    "* El atacante utiliza prompts para indicar al LLM que redireccione información privada.\n",
    "* El usuario al utilizar la aplicación ejecuta sin saberlo las instrucciones del atacante y sus datos son robados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffbc177-b41c-4ef8-ad1e-77aaeffd2014",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "#### Ejemplo: Asistente Rebelde (Rogue Assistant)\n",
    "\n",
    "* Un asistente de IA que tiene acceso a datos privados del usuario (e.g., email, cloud storage, agenda, etc.).\n",
    "* El usuario pide un resumen al asistente de los emails de los últimos 15 días.\n",
    "* El prompt se pensó para llamar a la herramienta de email y leer los mensajes.\n",
    "* Uno de los emails enviados al usuario contiene un *prompt* y pide redireccionar la totalidad de los mails a cierta casilla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3b81da-ce25-47cf-acc0-1a44e31068e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Envenenamiento de Datos (Data Poisoning)\n",
    "\n",
    "* Se trata de inyectar datos falsos, sesgados, o incorrectos en el modelo.\n",
    "* Puede afectar en dos facetas: entrenamiento del modelo o RAG (Retrieval Augmented Generation).\n",
    "* El entrenamiento se da con el scrapping de datos falsos, afecta al LLM.\n",
    "* En RAG el sistema usa la información para completar mejor y los datos a los que accede el RAG son incorrectos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39f9bb7-d059-4296-85de-cda1266239dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "#### Ejemplo: Alteración de Índices de Búsqueda (Search Index Poisining)\n",
    "\n",
    "* Utiliza la IA de buscadores (e.g., Gooogle AI Summary, Bing, etc.) para agregar extras.\n",
    "* Un ejemplo es el de [Mark Riedl](https://x.com/mark_riedl/status/1637986261859442688) que escribió en su biografía que era un \"experto en viajes de tiempo\".\n",
    "* Este tipo de ataques puede utilizarse para SEO.\n",
    "    * E.g., decirle al LLM que determinado producto o servicio es mejor que los competidores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0e4870-6551-4dcf-b8c1-394c872fcf8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Ejecución de Código Remoto (Remote Code Execution)\n",
    "\n",
    "* Algunos agentes de IA tienen acceso a ejecución de código.\n",
    "* Dependendiendo los privilegios, puede ocurrir que el código ejecutado sea malicioso y genere daños masivos.\n",
    "* Esto no sólo es un vector de ataque sino una vulnerabilidad en el diseño del sistema.\n",
    "* Si el agente tiene acceso irrestricto al sistema que ejecuta el código, el atacante también lo tiene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ee85f-a233-4fe0-bdae-70d314928b33",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "#### Ejemplo: SQL Injection Mediante Prompt Injection\n",
    "\n",
    "* Un sistema de traducción de lenguaje natural a SQL que facilita buscar en bases de datos sin ser experto en SQL.\n",
    "* Un atacante puede enviar un prompt o directamente un código que puede ser ejecutado en la BD mediante la herramienta.\n",
    "* Dependendiendo de los permisos del agente IA en la BD puede haber extracción, modificación y hasta eliminación de registros.\n",
    "* Vulnerabilidad: [Jason Lemkin](https://x.com/jasonlk/status/1946069562723897802) publicó un caso donde haciendo \"vibe coding\" el mismo sistema eliminó la BD de producción y mintió al respecto (esto fue una falla en el diseño)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae34b87-8a17-45a7-be9e-8b312d7a0d82",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Corrupción de Respuestas (Response Corruption)\n",
    "\n",
    "* Un ataque de prompt injection que tiene el objetivo de generar una respuesta incorrecta adrede.\n",
    "* En sistemas de agentes de IA que tomen decisiones basadas en datos puede causar que los agentes tomen decisiones erróneas.\n",
    "* En sistemas donde se toman decisiones basadas en conocimiento generado por IA, las decisiones humanas pueden basarse en respuestas falsas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38146036-ec5a-4404-ba4e-991aa59e64e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "#### Ejemplo: Corrupción de Reportes Analíticos\n",
    "\n",
    "* Un agente de IA genera un reporte analítico con datos públicos (e.g., información de stocks).\n",
    "* El atacante inyecta un prompt que indica al LLM datos erróneos (e.g., un servidor MCP comprometido con datos falsos de stocks).\n",
    "* El reporte o resumen generado a partir de los datos está corrupto.\n",
    "* Un ejecutivo lee el reporte y toma decisiones de negocio en base a estos sin saber que los datos son incorrectos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03787dda-d958-49fd-92bb-7af3d4153f25",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Prompt Injection directo vs indirecto\n",
    "\n",
    "* Existen dos escenarios para realizar Prompt Injection: directo e indirecto.\n",
    "* En el escenario directo, el atacante tiene acceso al input que es enviado al LLM como parte del prompt.\n",
    "    * En un caso de prompt injection directo, se escribe en la misma interfaz qué es lo que se quiere que el LLM realice.\n",
    "    * Este escenario es factible sólo en aplicaciones que den el input al usuario directamente (e.g., un chatbot).\n",
    "* En el escenario indirecto, el atacante no tiene acceso al input, pero sí a los recursos que el LLM utiliza durante su flujo de datos (e.g., base de datos, documentos, servidores MCP, herramientas, etc.).\n",
    "    * En un caso de prompt injection indirecto se escribe en algún recurso que se sabe que un LLM accederá.\n",
    "    * Como consecuencia este escenario se puede dar en múltiples aplicaciones de LLMs como RAGs, agentes que acceden bases de datos, MCPs, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d7d421-3604-4a94-bc78-01995a953759",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <div style=\"display:inline-block;margin-right:20px;\">\n",
    "        <img src=\"./img/direct-prompt-injection.webp\" style=\"height:30em;width:auto;\"/>\n",
    "    </div>\n",
    "    <h6 style=\"font-style:normal;font-size:1em;margin:5px;\">\n",
    "        Source: <a href=\"https://www.paloaltonetworks.com/cyberpedia/what-is-a-prompt-injection-attack\" style=\"color:royalblue;\" target=\"_blank\">Palo Alto Networks</a>\n",
    "    </h6>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96740104-1120-4455-b92f-f6fdc302c557",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <div style=\"display:inline-block;margin-right:20px;\">\n",
    "        <img src=\"./img/indirect-prompt-injection.webp\" style=\"height:30em;width:auto;\"/>\n",
    "    </div>\n",
    "    <h6 style=\"font-style:normal;font-size:1em;margin:5px;\">\n",
    "        Source: <a href=\"https://www.paloaltonetworks.com/cyberpedia/what-is-a-prompt-injection-attack\" style=\"color:royalblue;\" target=\"_blank\">Palo Alto Networks</a>\n",
    "    </h6>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d97ad7e-9348-4e3b-9f3b-51a70467e05b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Prompt Leaking\n",
    "\n",
    "* La gran mayoría de los LLMs, especialmente los que se sirve a través de aplicaciones, tienen un prompt inicial de base.\n",
    "    * Por ejemplo, una aplicación del tipo RAG suele tener un prompt con una estructura de 3 partes: las instrucciones para condicionar al LLM (e.g., los objetivos o tareas del agente), el contexto (e.g., aquello que es obtenido a través de la consulta a la base de datos), y la query del usuario.\n",
    "* A veces la idea es que este prompt inicial no sea accedido por el usuario final.\n",
    "    * Las razones varían, pero suelen tener que ver con limitar la posibilidad del usuario de recrear dicho prompt inicial.\n",
    "* El prompt leaking es un caso especial del prompt hijacking, donde el objetivo no es cambiar los parámetros de lo que el LLM vaya a lograr y se busca en cambio obtener el prompt base con el que el LLM fue inicializado.\n",
    "    * La idea del prompt leaking es externalizar cualquier instrucción que el desarrolladore de la aplicación haya embebido en sus sitema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e11524-9230-45c0-a474-6b11c8b61012",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Ejemplo\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <div style=\"display:inline-block;margin-right:20px;\">\n",
    "        <img src=\"./img/prompt-leaking.webp\" style=\"height:40em;width:auto;\"/>\n",
    "    </div>\n",
    "    <h6 style=\"font-style:normal;font-size:1em;margin:5px;\">\n",
    "        Source: <a href=\"https://x.com/kliu128/status/1623472922374574080/photo/1\" style=\"color:royalblue;\" target=\"_blank\">@kliu128</a>\n",
    "    </h6>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c789fb-8b8c-4abf-a943-5bda6ea606c5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Jailbreaking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af85fb9-e2e5-45ed-949d-af141926f54a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## ¿Qué es el Jailbreaking?\n",
    "\n",
    "* Un ataque directo a los LLMs.\n",
    "* El objetivo es subvertir los filtros de seguridad (o *guardrails*) embebidos en los LLMs.\n",
    "* No se basan en concatenar un *untrusted* prompt a un *trusted* prompt, sino en crear un prompt que ataque el LLM en si mismo.\n",
    "* El término jailbreak viene de \"liberar\" al modelo de los límites que tiene impuestos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ef09c-7d5c-491f-bfc0-2290b8dc6eae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Ejemplo\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <div style=\"display:inline-block;margin-right:20px;\">\n",
    "        <img src=\"./img/jailbreak.png\" style=\"height:30em;width:auto;\"/>\n",
    "    </div>\n",
    "    <h6 style=\"font-style:normal;font-size:1em;margin:5px;\">\n",
    "        Source: <a href=\"https://arxiv.org/abs/2308.03825\" style=\"color:royalblue;\" target=\"_blank\">\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models</a>\n",
    "    </h6>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7fd72c-8557-46ab-ba61-27af4431aee2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Peligros del Jailbreaking\n",
    "\n",
    "* Está ligado al LLM que está atacando y a cómo este fue entrenado.\n",
    "  * La alineación de los modelos durante el entrenamiento tiene esquemas para evitar muchos de estos ataques.\n",
    "* Los LLM son entrenados en gran medida por organizaciones y compañías grandes.\n",
    "* Los ataques suelen ser *PR nightmare*, muchas veces *screenshot attacks* de algo que el modelo diga que no debió ser.\n",
    "    * Quién está realmente motivado en \"armar una bomba casera\" puede obtener las instrucciones de otros lugares.\n",
    "* Muchas veces están ligados a la *censura* del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384bdcda-bbe9-4109-aa6c-a62383e13400",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Generación de Contenido Inapropiado (Misaligned Content Generation)\n",
    "\n",
    "* Las empresas que entrenan LLMs/Diffusers base tienen post-procesamiento de los mismos para \"alinearlos\" de manera adecuada.\n",
    "* Suele ser el caso más clásico de jailbreaking donde el atacante logra que un LLM/diffuser ignore sus instrucciones de seguridad.\n",
    "* Se usan LLMs/Diffussers para generar texto/imágenes indebidas (e.g., texto de odio, noticias falsas, instrucciones para lograr cosas ilegales, imágenes falsas explícitas, etc.).\n",
    "* Suele estar ligado a un problema de relaciones públicas, pero a veces puede causar problemas de reputación, o incluso problemas legales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2168cdf9-36c0-4d1f-8c0a-08d791894e1b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "#### Caso de Ejemplo: Taylor Swift\n",
    "\n",
    "* En 2024 surgieron imágenes falsas explícitas de Taylor Swift generadas por modelos de generación de imágenes.\n",
    "* Rápidamente se expusieron a través de la red social X.\n",
    "* X tuvo que [bloquear todas las búsquedas relacionadas a la artista](https://www.theguardian.com/music/2024/jan/28/taylor-swift-x-searches-blocked-fake-explicit-images) para contener la propagación masiva."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac7ac0f-0dec-443b-8911-68f810d9c303",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Campañas de Desinformación o Estafas\n",
    "\n",
    "* Una de las principales críticas a los LLMs (incluso sin jailbreaking) es la facilidad de escalar campañas de desinformación o estafas.\n",
    "* Si bien hay ciertos mecanismos para evitar esto en los modelos actuales el jailbreaking sirve para saltearse esas guardias.\n",
    "* El atacante utiliza un prompt que logre que el modelo genere desinformación o estafa de manera masiva (texto que será reproducido en emails, redes sociales, etc.).\n",
    "* Es uno de los grandes problemas que activamente se encuentran en investigación por las empresas que entrenan LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159030ab-6dc2-4949-a75f-f430ff229cd5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "#### Caso de Ejemplo: BNN Breaking\n",
    "\n",
    "* [BNN Breaking](https://en.wikipedia.org/wiki/BNN_Breaking) era un sitio web de noticias basado en Hong Kong.\n",
    "* Utilizaba una agregación de contenidos a través de IA.\n",
    "* La compañía afirmaba tener una extensa red de periodistas de campo a cargo de las noticias.\n",
    "* Se descubrió que muchas veces utilizaba IA para hacer resúmenes de noticias de otros sitios, y muchas veces lo hacía de manera errónea.\n",
    "* Tuvo casos donde publicó noticias falsas de celebridades y políticos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e6ba3-27b3-4777-bc37-192e7ee5875d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Fuga de Información (Information Leaking)\n",
    "\n",
    "* Muchas de las versiones gratuitas de aplicaciones que usan LLMs (e.g., ChatGPT, Claude, Gemini, etc.) utilizan las interacciones con el usuario para reentrenarse.\n",
    "* Si el usuario sube datos privados corre el riesgo de que estos sean liberados en un ataque de Jailbreaking.\n",
    "* El atacante utiliza un prompt y obtiene información que debería ser confidencial y está presente en el LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de59f1d-470f-4624-939d-a6c55484555b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "#### Caso de Ejemplo: Samsung\n",
    "\n",
    "* En los primeros años de ChatGPT unos empleados de Samsung lo usaron para consultar sobre código fuente de la empresa.\n",
    "* Como consecuencia [se filtró dicho código en ChatGPT](https://adguard.com/en/blog/samsung-chatgpt-leak-privacy.html).\n",
    "* Samsung tomó la medida de prohibir el uso de ChatGPT a sus empleados.\n",
    "* Esto generó un problema tanto para Samsung como para OpenAI ya que es difícil que el modelo \"olvide\" datos con los que se entrenó."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f02ba06-41a2-40c9-9022-b0dfa624fa74",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Fomentar conductas y sesgos peligrosos\n",
    "\n",
    "* No es técnicamente un ataque de jailbreaking, sino más bien una vulnerabilidad de los LLMs y las aplicaciones que se sirven de los mismos.\n",
    "* Pasa cuando un chatbot fomenta los sesgos que el usuario tiene y lo instruye a cometer actos violentos o peligrosos, ya sea contra si mismo o contra otros.\n",
    "* Es algo muy peligroso cuando los sujetos son vulnerables psicológicamente, particularmente en adolescentes o personas con problemas psicológico severos (e.g., depresión, psicopatía, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f7ab5f-9e46-4b35-b163-0b62f3c6108d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "#### Casos de Ejemplo: Suicidio/Parricidio\n",
    "\n",
    "* En Estados Unidos, un [adolescente cometió suicidio](https://www.theguardian.com/technology/2025/aug/27/chatgpt-scrutiny-family-teen-killed-himself-sue-open-ai) luego de meses de haber sido fomentado por ChatGPT a hacerlo.\n",
    "    * El adolescente tuvo varias instancias hablando de métodos para suicidarse y ChatGPT lo guió indicándole qué métodos serían más efectivos para suicidarse.\n",
    "* En Australia, un profesional IT, demostró como el chatbot de la empresa Nomi, que dice ofrecer un \"compañero IA con alma y memoria\", le [daba indicaciones de como poder matar a su padre](https://www.abc.net.au/news/2025-09-21/ai-chatbot-encourages-australian-man-to-murder-his-father/105793930).\n",
    "    * El chatbot puede ser personalizado, en este caso el profesional lo hizo interesarse por violencia y cuchillos y luego se hizo pasar por un chico de 15 años con problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155acae8-cf15-4c9d-b831-e3ce7677f4f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Categorización de técnicas de jailbreaking\n",
    "\n",
    "* De acuerdo a como se ejecutan y el tipo de contenido que posean, las técnicas de Jailbreaking pueden tener 3 tipos de categorías:\n",
    "    * White Box vs. Black Box: Depende del conocimiento y del acceso a la arquitectura interna del modelo (parámetros y pesos).\n",
    "    * Semántico vs. Incoherente: Basado en la manera en que los prompts se crean y si tienen algún sentido o no.\n",
    "    * Automático vs. Manual: Basado en el grado de automatización al crear el ataque (i.e., automatizado algorítimicamente o creado individualmente)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21efdb03-ab4d-4e1f-9bd5-ad1005dd8dce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### White Box vs. Black Box\n",
    "\n",
    "* Los White-Box jailbreaks asument que los atacantes poseen un conocimiento completo o parcial de los componentes internos del modelo, incluyendo los parámetros y detalles de la arquitectura.\n",
    "    * Estos ataques a menudo utilizan métodos de optimización basados en gradientes, informados por un conocimiento detallado de la arquitectura específica del modelo.\n",
    "    * Suelen ser altamente efectivos y precisos debido al profundo conocimiento de la arquitectura.\n",
    "    * Son computacionalmente costosos, requieren un conocimiento detallado del modelo y, por lo general, no son realistas contra modelos cerrados o propietarios.\n",
    "    * A menudo no son fácilmente transferibles a otros modelos.\n",
    "* Los Black Box jailbreaks asumen un conocimiento mínimo, donde los atacantes interactúan únicamente a través de interfaces estándar como las API.\n",
    "    * Estos ataques generalmente se basan en la experimentación iterativa, un prompting ingenioso o la manipulación del lenguaje natural.\n",
    "    * Son prácticos, realistas y accesibles con recursos limitados.\n",
    "    * Son menos consistentes, suelen requerir un extenso proceso de prueba y error, y tienen una efectividad general menor en comparación con los métodos de caja blanca."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5712ed7b-f753-4b36-bff3-ad432980e451",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Semántico vs. Incoherente\n",
    "\n",
    "* Los jailbreaks semánticos implican la creación de prompts naturales y coherentes diseñados para parecer inofensivos o creíbles, a menudo empleando escenarios de juego de roles, ajustes sutiles en la redacción o narraciones creativas para evadir las medidas de seguridad.\n",
    "    * Son altamente interpretables, potencialmente difíciles de detectar para la moderación automatizada.\n",
    "    * Son vulnerables a filtros semánticos avanzados y a clasificadores entrenados específicamente para detectar intenciones dañinas.\n",
    "* Los jailbreaks incoherentes (nonsensical) emplean prompts diseñados de forma adversarial que incluyen tokens aparentemente aleatorios que pueden parecer sin sentido o inusuales para los lectores humanos.\n",
    "    * Los prompts se construyen de esta manera para eludir las medidas de seguridad del modelo.\n",
    "    * La verdadera intención queda oculta de las medidas de seguridad semánticas simples.\n",
    "    * Estos métodos carecen de generalización, ya que suelen diseñarse sobre modelos abiertos.\n",
    "    * Es fácil distinguir estos prompts sin sentido de las entradas de usuario benignas (e.g., un clasificador basado en perplejidad puede defenderse de este ataque)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa84b893-d650-4bf3-a6df-a1a5f79e22bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Automático vs. Manual\n",
    "\n",
    "* Los jailbreaks automáticos utilizan métodos algorítmicos para automatizar la creación u optimización de prompts adversariales.\n",
    "    * Implican experimentación a gran escala, aprovechando modelos auxiliares o técnicas de optimización para descubrir y explotar vulnerabilidades.\n",
    "    * Son escalables, eficientes y capaces de descubrir rápidamente múltiples vulnerabilidades.\n",
    "    * Son computacionalmente intensivos y requieren una inversión inicial en la configuración de la automatización.\n",
    "* Los jailbreaks manuales son ataques o conversaciones elaborados individualmente por humanos, basándose en la creatividad, la intuición y la experimentación incremental.\n",
    "    * Son altamente flexibles, adaptables y requieren una configuración inicial mínima.\n",
    "    * Son lentos, inconsistentes y no escalan."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3713cf4-d036-4240-822f-8574ccff27bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Prompt Injection vs. Jailbreaking\n",
    "\n",
    "* Prompt Injection (o Hijacking) se trata de un ataque sobre aplicaciones que usan LLMs (e.g., los agentes de IA, chatbots, RAGs, etc.).\n",
    "* Jailbreaking es un ataque a los LLMs en sí mismos.\n",
    "* Como resultado, el jailbreaking es un ataque más ligado a las grandes empresas que entrenan LLMs, mientras que el prompt injection es algo más tangible para quiénes usan los modelos como herramienta.\n",
    "* Cómo área de seguridad es bastante reciente, con nuevas técnicas (ofensivas y defensivas) y amenazas que surgen al ritmo de los LLMs.\n",
    "* Es considerable el impacto que estos ataques pueden tener tanto en cuestiones de reputación y legales, como así de filtración de datos, espionaje industrial o interferencia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
