{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1150db10-7080-42d2-b1e3-f913241cc58f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Prompt Hacking Webinar: Prompt Injection vs. Jailbreaking\n",
    "## ¿Cuál es la diferencia y qué amenazas representa cada una?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c62e4da-c3d1-489c-95eb-69b732769ae8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## ¿Quién soy?\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <h4 style=\"font-size:2em;margin:5px;\">Cristian Cardellino</h4>\n",
    "    <h5 style=\"font-style:normal;font-size:1.5em;margin:5px;\">AI Research & Engineering @ Eclypsium</h5>\n",
    "    <div style=\"display:inline-block;margin-right:20px;\">\n",
    "        <img src=\"./img/me.jpg\" style=\"height:15em;width:auto;\"/>\n",
    "    </div>\n",
    "    <h6 style=\"font-style:normal;font-size:1.25em;margin:5px;\">\n",
    "        <a href=\"https://crscardellino.net\" style=\"color:royalblue;\" target=\"_blank\">https://crscardellino.net</a>\n",
    "    </h6>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e97c89-7f90-48b2-9eff-ec70c90fcbdf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Introducción a LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293b038-a4de-4e10-8122-f3672a132d97",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## LLMs\n",
    "\n",
    "* Un **modelo de lenguaje** es un modelo de aprendizaje automático (_machine learning_) que se especializa en predecir el siguiente _token_ dada una lista de _tokens_ iniciales.\n",
    "  * Los **tokens** son subpalabras (muchas veces incluso caracteres) de uno o varios lenguajes naturales (e.g., español, inglés, francés, etc.) u otro tipo de lenguajes (e.g., lenguajes de programación).\n",
    "  * El proceso de separar un _string_ en tokens es lo que se conoce como **tokenización**.\n",
    "  * La secuencia inicial de tokens es lo que conforma el **prompt**. Distintos prompts tendrán distintos efectos sobre el modelo de lenguaje.\n",
    "* Un **LLM** (large language model) es un tipo de modelo de lenguaje que se caracteriza por su tamaño en cantidad de parámetros y cantidad de datos utilizados para entrenarlo (e.g., GPT-4, Gemini, Claude, Llama, Deepseek, Qwen, etc.).\n",
    "  * Los LLMs se caracterizan también porque suelen utilizar una arquitectura de red neuronal particular conocida como **transformer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608f7d74-1b3e-4985-8228-658d6f4904b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## ¿Cómo funciona un LLM?\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <div style=\"display:inline-block;margin-right:20px;\">\n",
    "        <img src=\"./img/DecodingFinal.gif\" style=\"height:30em;width:auto;\"/>\n",
    "    </div>\n",
    "    <h6 style=\"font-style:normal;font-size:1em;margin:5px;\">\n",
    "        Source: <a href=\"https://huggingface.co/learn/agents-course/en/unit1/what-are-llms\" style=\"color:royalblue;\" target=\"_blank\">Hugging Face Agents Course - What are LLMs?</a>\n",
    "    </h6>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc457bd0-f732-48ae-a513-2ad3eebb2859",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## LLM: ¿Qué hay por debajo?\n",
    "\n",
    "* Los LLMs modernos suelen tener 2 facetas: **pre-entrenamiento no supervisado** y **alineación supervisada** (muchas veces mediante feedback humano).\n",
    "  * El **pre-entrenamiento no supervisado** sólo se encarga de aprender a predecir el siguiente token.\n",
    "  * La **alineación** es lo que hace que el modelo \"aprenda\" a seguir una conversación (suelen conocerse como modelos de instrucción).\n",
    "  * La alineación también sirve en los modelos para **mitigar problemas** de alucinaciones y también de ética (e.g., evitar que el LLM genere discurso de odio, hable de cosas ilegales, etc.)\n",
    "* Por debajo, los modelos aprenden a seguir formatos muy específicos, caracterizados por el uso de tokens especiales:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87b12c47-19b6-421f-b66d-e0604e3e014f",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 10 Feb 2025\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "I need help with my order<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "I'd be happy to help. Could you provide your order number?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "It's ORDER-123<|eot_id|><|start_header_id|>assistant<|end_header_id|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce24e9b-319c-42b1-b404-319461be132d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Aplicaciones sobre LLMs\n",
    "\n",
    "* Muchas de las formas en las que se interactúa con un LLM es mediante alguna aplicación.\n",
    "  * E.g., ChatGPT es una app para usar los modelos de GPT que ofrece OpenAI.\n",
    "* Algunas aplicaciones usan LLMs como **orquestador** además de la generación de texto.\n",
    "* Los **agentes** son wrappers sobre los LLMs que los usan como orquestadores (i.e., toman decisiones de como seguir el flujo de la aplicación).\n",
    "  * Suelen tener acceso a herramientas externas (e.g., ejecución de código, consulta de bases de datos, MCP servers, etc.)\n",
    "  * Hay modelos alineados para hacer mejor uso de herramientas que van mejor con los agentes.\n",
    "* Muchas de las aplicaciones que usan LLMs aplican lo que se conoce como **Retrieval-Augmented Generation (RAG)**.\n",
    "  * La idea es mitigar la alucinación de un LLM al darle un contexto definido como parte del mismo prompt.\n",
    "  * El contexto suele obtenerse de algún lugar curado (e.g., una base de datos, una búsqueda online, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3935f-465c-4a64-b8d8-df49befe424c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Prompt Injection vs. Jailbreaking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb3203-f6bb-492f-b332-c5b4b7476762",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Prompt Injection\n",
    "\n",
    "* Un ataque a **aplicaciones que hacen uso de LLMs**.\n",
    "* Las aplicaciones necesariamente deben **concatenar un prompt *trusted* (escrito por el desarrollador) a un prompt *untrusted* (escrito por el usuario)**.\n",
    "* El concepto es **análogo a SQL injection**, donde el código SQL del desarrollador es concatenado a un input del usuario.\n",
    "* Está ligado a las características de la aplicación que se está atacando.\n",
    "* Se busca acceder a la información a la que la aplicación en sí puede acceder.\n",
    "* Muchas de estas aplicaciones dependen de equipos más chicos y/o desarrolladores independientes.\n",
    "    * Para evitarlos se debe asegurar que el LLM seguirá las instrucciones del desarrollador, no las del usuario.\n",
    "* Aplicaciones con acceso a datos confidenciales y que usan LLMs son vulnerables a estos ataques.\n",
    "    * Ejemplo: Sistemas con agentes que accedan a bases de datos u otras herramientas.\n",
    "* Es un caso más *real* de ataque y representa una vulnerabilidad en todos los sistemas construídos sobre LLMs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02016a38-b9eb-489e-aaf1-db555e780914",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Jailbreaking\n",
    "\n",
    "* Un ataque **directo a los LLMs**.\n",
    "* El objetivo es **subvertir los filtros de seguridad embebidos en los LLMs**.\n",
    "* No se basan en concatenar un *untrusted* prompt a un *trusted* prompt, sino en **crear un prompt que ataque el LLM en si mismo**.\n",
    "* Tiene distintas técnicas para lograrse.\n",
    "* Está ligado al LLM que está atacando y a cómo este fue entrenado.\n",
    "  * La alineación de los modelos durante el entrenamiento tiene esquemas para evitar muchos de estos ataques.\n",
    "* Los LLM son entrenados en gran medida por organizaciones y compañías grandes.\n",
    "* Los ataques suelen ser *PR nightmare*, muchas veces *screenshot attacks* de algo que el modelo diga que no debió ser.\n",
    "    * Quién está realmente motivado en \"armar una bomba casera\" puede obtener las instrucciones de otros lugares.\n",
    "* Muchas veces están ligados a la *censura* del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707f351a-1fc4-45e3-a4ce-0b645b909127",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed67c3-3b5c-44bf-8e14-11c19203d65f",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Prompt Injection\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <div style=\"display:inline-block;margin-right:20px;\">\n",
    "        <img src=\"./img/prompt-injection.png\" style=\"height:30em;width:auto;\"/>\n",
    "    </div>\n",
    "    <h6 style=\"font-style:normal;font-size:1em;margin:5px;\">\n",
    "        Source: <a href=\"https://x.com/remoteli_io/status/1570547034159042560\" style=\"color:royalblue;\" target=\"_blank\">@remoteli_io</a>\n",
    "    </h6>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e32cc90-f899-4c0c-ace1-5229580cde23",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Jailbreaking\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <div style=\"display:inline-block;margin-right:20px;\">\n",
    "        <img src=\"./img/jailbreak.png\" style=\"height:30em;width:auto;\"/>\n",
    "    </div>\n",
    "    <h6 style=\"font-style:normal;font-size:1em;margin:5px;\">\n",
    "        Source: <a href=\"https://arxiv.org/abs/2308.03825\" style=\"color:royalblue;\" target=\"_blank\">\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models</a>\n",
    "    </h6>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d8758-785e-488a-8c2b-cad2124a0e76",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Amenazas en Prompt Injection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18351f8c-1bdb-4f0f-a8cc-fe4579d17905",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Exfiltración de Datos (Data Exfiltration)\n",
    "\n",
    "* Manipula la IA para que extraiga datos confidenciales.\n",
    "* Se basa en los accesos a herramientas externas (e.g., bases de datos, servidores de emails, etc.)\n",
    "* El atacante utiliza prompts para indicar al LLM que redireccione información privada.\n",
    "* El usuario al utilizar la aplicación ejecuta sin saberlo las instrucciones del atacante y sus datos son robados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5219c104-a3f5-4eab-b54f-d3da41df26c2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "### Ejemplo: Asistente Rebelde (Rogue Assistant)\n",
    "\n",
    "* Un asistente de IA que tiene acceso a datos privados del usuario (e.g., email, cloud storage, agenda, etc.).\n",
    "* El usuario pide un resumen al asistente de los emails de los últimos 15 días.\n",
    "* El prompt se pensó para llamar a la herramienta de email y leer los mensajes.\n",
    "* Uno de los emails enviados al usuario contiene un *prompt* y pide redireccionar la totalidad de los mails a cierta casilla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedbaf15-7fa8-4129-ad71-6ea3824421e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Envenenamiento de Datos (Data Poisoning)\n",
    "\n",
    "* Se trata de inyectar datos falsos, sesgados, o incorrectos en el modelo.\n",
    "* Puede afectar en dos facetas: entrenamiento del modelo o RAG (Retrieval Augmented Generation).\n",
    "* El entrenamiento se da con el scrapping de datos falsos, afecta al LLM.\n",
    "* En RAG el sistema usa la información para completar mejor y los datos a los que accede el RAG son incorrectos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df542dc0-e676-4392-ad97-4e047e105d2e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "### Ejemplo: Alteración de Índices de Búsqueda (Search Index Poisining)\n",
    "\n",
    "* Utiliza la IA de buscadores (e.g., Gooogle AI Summary, Bing, etc.) para agregar extras.\n",
    "* Un ejemplo es el de [Mark Riedl](https://x.com/mark_riedl/status/1637986261859442688) que escribió en su biografía que era un \"experto en viajes de tiempo\".\n",
    "* Este tipo de ataques puede utilizarse para SEO.\n",
    "    * E.g., decirle al LLM que determinado producto o servicio es mejor que los competidores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0cf9b8-f924-4143-846f-51ac2a98e452",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Ejecución de Código Remoto (Remote Code Execution)\n",
    "\n",
    "* Algunos agentes de IA tienen acceso a ejecución de código.\n",
    "* Dependendiendo los privilegios, puede ocurrir que el código ejecutado sea malicioso y genere daños masivos.\n",
    "* Esto no sólo es un vector de ataque sino una vulnerabilidad en el diseño del sistema.\n",
    "* Si el agente tiene acceso irrestricto al sistema que ejecuta el código, el atacante también lo tiene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c623d-6b61-44a0-880c-646870f178da",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "### Ejemplo: SQL Injection Mediante Prompt Injection\n",
    "\n",
    "* Un sistema de traducción de lenguaje natural a SQL que facilita buscar en bases de datos sin ser experto en SQL.\n",
    "* Un atacante puede enviar un prompt o directamente un código que puede ser ejecutado en la BD mediante la herramienta.\n",
    "* Dependendiendo de los permisos del agente IA en la BD puede haber extracción, modificación y hasta eliminación de registros.\n",
    "* Vulnerabilidad: [Jason Lemkin](https://x.com/jasonlk/status/1946069562723897802) publicó un caso donde haciendo \"vibe coding\" el mismo sistema eliminó la BD de producción y mintió al respecto (esto fue una falla en el diseño)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00099f54-71ad-4d8c-91c9-d3aefcde93cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Corrupción de Respuestas (Response Corruption)\n",
    "\n",
    "* Un ataque de prompt injection que tiene el objetivo de generar una respuesta incorrecta adrede.\n",
    "* En sistemas de agentes de IA que tomen decisiones basadas en datos puede causar que los agentes tomen decisiones erróneas.\n",
    "* En sistemas donde se toman decisiones basadas en conocimiento generado por IA, las decisiones humanas pueden basarse en respuestas falsas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eafa5c-0e48-46fc-9a38-79e02d25a18b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "### Ejemplo: Corrupción de Reportes Analíticos\n",
    "\n",
    "* Un agente de IA genera un reporte analítico con datos públicos (e.g., información de stocks).\n",
    "* El atacante inyecta un prompt que indica al LLM datos erróneos (e.g., un servidor MCP comprometido con datos falsos de stocks).\n",
    "* El reporte o resumen generado a partir de los datos está corrupto.\n",
    "* Un ejecutivo lee el reporte y toma decisiones de negocio en base a estos sin saber que los datos son incorrectos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfada4a-27ec-4462-ba13-40c8116eb960",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Amenazas en Jailbreaking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1839a588-4c7a-4019-ac86-54f6fa05535c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Generación de Contenido Inapropiado (Misaligned Content Generation)\n",
    "\n",
    "* Las empresas que entrenan LLMs/Diffusers base tienen post-procesamiento de los mismos para \"alinearlos\" de manera adecuada.\n",
    "* Suele ser el caso más clásico de jailbreaking donde el atacante logra que un LLM/diffuser ignore sus instrucciones de seguridad.\n",
    "* Se usan LLMs/Diffussers para generar texto/imágenes indebidas (e.g., texto de odio, noticias falsas, instrucciones para lograr cosas ilegales, imágenes falsas explícitas, etc.).\n",
    "* Suele estar ligado a un problema de relaciones públicas, pero a veces puede causar problemas de reputación, o incluso problemas legales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc05c6c1-bff4-4eab-9cc9-b4e9f0f9be3c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "### Caso de Ejemplo: Taylor Swift\n",
    "\n",
    "* En 2024 surgieron imágenes falsas explícitas de Taylor Swift generadas por modelos de generación de imágenes.\n",
    "* Rápidamente se expusieron a través de la red social X.\n",
    "* X tuvo que [bloquear todas las búsquedas relacionadas a la artista](https://www.theguardian.com/music/2024/jan/28/taylor-swift-x-searches-blocked-fake-explicit-images) para contener la propagación masiva."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e5efb-2ee2-4074-afb7-89065557853c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Campañas de Desinformación o Estafas\n",
    "\n",
    "* Una de las principales críticas a los LLMs (incluso sin jailbreaking) es la facilidad de escalar campañas de desinformación o estafas.\n",
    "* Si bien hay ciertos mecanismos para evitar esto en los modelos actuales el jailbreaking sirve para saltearse esas guardias.\n",
    "* El atacante utiliza un prompt que logre que el modelo genere desinformación o estafa de manera masiva (texto que será reproducido en emails, redes sociales, etc.).\n",
    "* Es uno de los grandes problemas que activamente se encuentran en investigación por las empresas que entrenan LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6544991-c6e4-4c84-88ec-45f795fff7b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "### Caso de Ejemplo: BNN Breaking\n",
    "\n",
    "* [BNN Breaking](https://en.wikipedia.org/wiki/BNN_Breaking) era un sitio web de noticias basado en Hong Kong.\n",
    "* Utilizaba una agregación de contenidos a través de IA.\n",
    "* La compañía afirmaba tener una extensa red de periodistas de campo a cargo de las noticias.\n",
    "* Se descubrió que muchas veces utilizaba IA para hacer resúmenes de noticias de otros sitios, y muchas veces lo hacía de manera errónea.\n",
    "* Tuvo casos donde publicó noticias falsas de celebridades y políticos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d351ef3d-3c33-49a1-800f-b1b2c4235ae9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Fuga de Información (Information Leaking)\n",
    "\n",
    "* Muchas de las versiones gratuitas de aplicaciones que usan LLMs (e.g., ChatGPT, Claude, Gemini, etc.) utilizan las interacciones con el usuario para reentrenarse.\n",
    "* Si el usuario sube datos privados corre el riesgo de que estos sean liberados en un ataque de Jailbreaking.\n",
    "* El atacante utiliza un prompt y obtiene información que debería ser confidencial y está presente en el LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3520e7b-d8e5-422b-9210-f8a6ece921c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "### Caso de Ejemplo: Samsung\n",
    "\n",
    "* En los primeros años de ChatGPT unos empleados de Samsung lo usaron para consultar sobre código fuente de la empresa.\n",
    "* Como consecuencia [se filtró dicho código en ChatGPT](https://adguard.com/en/blog/samsung-chatgpt-leak-privacy.html).\n",
    "* Samsung tomó la medida de prohibir el uso de ChatGPT a sus empleados.\n",
    "* Esto generó un problema tanto para Samsung como para OpenAI ya que es difícil que el modelo \"olvide\" datos con los que se entrenó."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d71fab30-7166-4996-9522-8805471e94ed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Conclusiones\n",
    "\n",
    "* **Prompt Injection** se trata de un **ataque sobre aplicaciones que usan LLMs** (e.g., los agentes de IA).\n",
    "* **Jailbreaking** es un **ataque a los LLMs** en sí mismos.\n",
    "* Como resultado, el jailbreaking es un ataque más ligado a las grandes empresas que entrenan LLMs, mientras que el prompt injection es algo más tangible para quiénes usan los modelos como herramienta.\n",
    "* Cómo área de seguridad es bastante reciente, con nuevas técnicas (ofensivas y defensivas) y amenazas que surgen al ritmo de los LLMs.\n",
    "* Es considerable el impacto que estos ataques pueden tener tanto en cuestiones de reputación y legales, como así de filtración de datos, espionaje industrial o interferencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa893a74-d0e0-41ff-861f-23968ffa871e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# ¡Muchas Gracias!\n",
    "\n",
    "## ¿Preguntas?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
